{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import os\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from pickle import dump, load\n",
        "import numpy as np\n",
        "from keras.utils import plot_model\n",
        "from keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras.utils import img_to_array , load_img\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def load_document(filename):\n",
        "  file = open(filename,'r')\n",
        "  data = file.read()\n",
        "  file.close()\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def load_image_captions(filename):\n",
        "  file = load_document(filename)\n",
        "  img_captions = file.split('\\n')\n",
        "  del img_captions[0]\n",
        "  descriptions = {}\n",
        "  rearranged = []\n",
        "\n",
        "  for img_caption in img_captions[:-1]:\n",
        "    index = img_caption.find(',')\n",
        "    img_caption = img_caption[:index] + \"\\t\" + img_caption[index+1:] \n",
        "    rearranged.append(img_caption)\n",
        "  img_captions = rearranged\n",
        "  \n",
        "  for img_caption in img_captions:\n",
        "    image , caption = img_caption.split('\\t')\n",
        "    if image not in descriptions:\n",
        "      descriptions[image] = [caption]\n",
        "    else:\n",
        "      descriptions[image].append(caption)\n",
        "  return descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def cleaning_captions(captions):\n",
        "  table = str.maketrans('','',string.punctuation)\n",
        "  for image,img_captions in captions.items():\n",
        "    for index, image_caption in enumerate(img_captions):\n",
        "      image_caption.replace(\"-\",\"\")\n",
        "      caption_words = image_caption.split()\n",
        "      caption_words = [word.lower() for word in caption_words]\n",
        "      caption_words = [word.translate(table) for word in caption_words]\n",
        "      caption_words = [word for word in caption_words if len(word)>1]\n",
        "      caption_words = [word for word in caption_words if word.isalpha()]\n",
        "      image_caption = ' '.join(caption_words)\n",
        "      captions[image][index] = image_caption\n",
        "  return captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def vocabulary(image_captions):\n",
        "  vocabulary = set()\n",
        "  for image in image_captions.keys():\n",
        "    [vocabulary.update(caption.split()) for caption in image_captions[image]]\n",
        "  return vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def save_to_file(image_captions,filename):\n",
        "  image_captions_list = list()\n",
        "  for image,captions in image_captions.items():\n",
        "    for caption in captions:\n",
        "      image_captions_list.append(image + '\\t' + caption)\n",
        "  data = \"\\n\".join(image_captions_list)\n",
        "  file = open(filename,'w')\n",
        "  file.write(data)\n",
        "  file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "filename = \"/content/captions.txt\"\n",
        "image_caption = load_image_captions(filename)\n",
        "image_caption\n",
        "image_captions = cleaning_captions(image_caption)\n",
        "language_vocabulary = vocabulary(image_captions)\n",
        "save_to_file(image_captions,'image_captions.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def extract_features(file_location):\n",
        "  model = Xception(include_top = False,pooling = 'avg')\n",
        "  features = {}\n",
        "  for image in os.listdir(file_location):\n",
        "    img = file_location + '/' + image \n",
        "    img = Image.open(img)\n",
        "    img = img.resize((299,299))\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = img/127.5\n",
        "    img = img - 1.0\n",
        "    \n",
        "    feature = model.predict(img)\n",
        "    features[image] = feature\n",
        "  return features\n",
        "      \n",
        "\n",
        "file_location = \"/content/Flicker8k_Dataset\"\n",
        "features = extract_features(file_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "dump(features,open(\"Imagefeatures.p\",\"wb\"))\n",
        "len(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def load_imgNames(description):\n",
        "    return list(description.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def load_clean_captions(filename,imgNames):\n",
        "    imgCaptions = load_document(filename)\n",
        "    imgCaptions = imgCaptions.split('\\n')\n",
        "    descriptions = {}\n",
        "    for imgCaption in imgCaptions:\n",
        "        words = imgCaption.split()\n",
        "        if len(words) < 1:\n",
        "            continue\n",
        "        \n",
        "        imgName = words[0]\n",
        "        imgCap = words[1:]\n",
        "        \n",
        "        if imgName in imgNames:\n",
        "            if imgName not in descriptions:\n",
        "                descriptions[imgName] = []\n",
        "            \n",
        "            imgCap = \"<start> \" + \" \".join(imgCap) + \" <end>\"\n",
        "            descriptions[imgName].append(imgCap)\n",
        "    \n",
        "    return descriptions\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def load_imgFeatures(imgNames):\n",
        "  img_features = load(open('Imagefeatures.p','rb'))\n",
        "  features = {k:img_features[k] for k in imgNames}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "imgNames = load_imgNames(image_caption)[:-1]\n",
        "clean_ImgCaptions = load_clean_captions('/content/image_captions.txt',imgNames)\n",
        "load_imgFeatures(imgNames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def list_captions(imgCaptions):\n",
        "    imgNames = imgCaptions.keys()\n",
        "    captions_list = []\n",
        "    for imgName in imgNames:\n",
        "        [captions_list.append(imgCaption) for imgCaption in imgCaptions[imgName]]\n",
        "    return captions_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def create_Tokenizer(imgCaptions):\n",
        "    captions_list = list_captions(imgCaptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(captions_list)\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "tokenizer = create_Tokenizer(clean_ImgCaptions)\n",
        "dump(tokenizer,open('tokens.p','wb'))\n",
        "vocabulary_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "vocabulary_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def max_length(imgCaptions):\n",
        "    captions_list = list_captions(imgCaptions)\n",
        "    return max(len(imgCaption.split()) for imgCaption in captions_list)\n",
        "\n",
        "maxlength = max_length(image_caption)\n",
        "maxlength"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def create_sequences(tokenizer, max_length, img_captions, feature):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    for img_caption in img_captions:\n",
        "        \n",
        "        seq = tokenizer.texts_to_sequences([img_caption])[0]\n",
        "        for i in range(1, len(seq)):\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocabulary_size)[0]\n",
        "            \n",
        "            X1.append(feature)\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "def data_generator(imgCaptions, features, tokenizer, max_length):\n",
        "    while 1:\n",
        "        for key, img_captions in imgCaptions.items():\n",
        "            feature = features[key][0]\n",
        "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, img_captions, feature)\n",
        "            yield [[input_image, input_sequence], output_word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "[a,b],c = next(data_generator(clean_ImgCaptions, features, tokenizer, maxlength))\n",
        "a.shape, b.shape, c.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def define_model(vocabulary_size, max_length):\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "    fe3 = Dense(128, activation='relu')(fe2)\n",
        "\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocabulary_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.4)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "    decoder1 = concatenate([fe3, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    decoder3 = Dense(128, activation='relu')(decoder2)\n",
        "    outputs = Dense(vocabulary_size, activation='softmax')(decoder3)\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr = 0.002))\n",
        "    print(model.summary())\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "model = define_model(vocabulary_size, maxlength)\n",
        "epochs = 50\n",
        "steps = len(clean_ImgCaptions)\n",
        "for i in range(epochs):\n",
        "    generator = data_generator(clean_ImgCaptions, features, tokenizer, maxlength)\n",
        "    model.fit(generator, epochs=1, steps_per_epoch= steps, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def word_for_id(integer, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "     if index == integer:\n",
        "         return word\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    in_text = 'start'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        pred = model.predict([photo,sequence], verbose=0)\n",
        "        pred = np.argmax(pred)\n",
        "        word = word_for_id(pred, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'end':\n",
        "            break\n",
        "    return in_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "tokenizer = load(open(\"tokens.p\",\"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def extract_features(filename, model):\n",
        "        try:\n",
        "            image = Image.open(filename)\n",
        "        except:\n",
        "            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n",
        "        image = image.resize((299,299))\n",
        "        image = np.array(image)\n",
        "        # for images that has 4 channels, we convert them into 3 channels\n",
        "        if image.shape[2] == 4: \n",
        "            image = image[..., :3]\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = image/127.5\n",
        "        image = image - 1.0\n",
        "        feature = model.predict(image)\n",
        "        return feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "xception_model = Xception(include_top=False, pooling=\"avg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "filePath = '/content/Flicker8k_Dataset'\n",
        "for i in range(0,100):\n",
        "  filename = filePath + '/' + os.listdir(filePath)[i]\n",
        "  img_feature = extract_features(filename,xception_model)\n",
        "\n",
        "  img_description = generate_desc(model,tokenizer,img_feature,maxlength)\n",
        "  image = Image.open(filename)\n",
        "  plt.imshow(image)\n",
        "  plt.show()\n",
        "  print(img_description)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
